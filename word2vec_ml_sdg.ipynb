{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_ml_sdg.ipynb",
      "provenance": [],
      "mount_file_id": "16LAQVxXNgIh6DoI0ikh8SPj8TER90Nwb",
      "authorship_tag": "ABX9TyOXi4PdV/goGp/7xHf4NIhG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyawalia/ai-powered-file-management/blob/main/word2vec_ml_sdg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Mk3AK46bWf",
        "outputId": "2a793d77-596b-4235-c982-5cc5d4d90c72"
      },
      "source": [
        "!pip install --upgrade keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp5cCz0xEr-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70d4695-0f42-4eae-f0ab-f7d083e53d7d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, hamming_loss, accuracy_score\n",
        "from keras import optimizers\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, Flatten\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "# Conv\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "# LSTM\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, SpatialDropout1D, Bidirectional, GRU, LSTM\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jsXSA7VE1RZ"
      },
      "source": [
        "base_dir = \"/content/drive/My Drive/sdg/\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nolYEeAUDhBY"
      },
      "source": [
        "TEXT_DATA_DIR = f\"{base_dir}dataset/sdg_tag.csv\"\n",
        "EMBEDDINGS_DIR = f\"{base_dir}embeddings/word2vec/\"\n",
        "CROSS_FOLDS = f\"{base_dir}dataset/cross_validation/\"\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "labels_index = [str(i) for i in range(1,18)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cw-mvwhLKST"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQLoPjXZLMGi"
      },
      "source": [
        "def cleanHtml(sentence):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
        "    return cleantext\n",
        "\n",
        "\n",
        "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    cleaned = cleaned.strip()\n",
        "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def keepAlpha(sentence):\n",
        "    alpha_sent = \"\"\n",
        "    for word in sentence.split():\n",
        "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
        "        alpha_sent += alpha_word\n",
        "        alpha_sent += \" \"\n",
        "    alpha_sent = alpha_sent.strip()\n",
        "    return alpha_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw14RnvXMGcl"
      },
      "source": [
        "labelled = pd.read_csv(TEXT_DATA_DIR)\n",
        "labelled['description'] = labelled['description'].str.lower()\n",
        "labelled['description'] = labelled['description'].apply(cleanHtml)\n",
        "labelled['description'] = labelled['description'].apply(cleanPunc)\n",
        "labelled['description'] = labelled['description'].apply(keepAlpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEbylCgvPYyO"
      },
      "source": [
        "vocab = Counter()\n",
        "\n",
        "# Masked for training and valid. This will be part of the vocab and index\n",
        "texts = [word_tokenize(t.lower()) for t in labelled.description]\n",
        "\n",
        "# Same masked vocab, embeddings and index\n",
        "for text in texts:\n",
        "    vocab.update(text)    \n",
        "model = Word2Vec(texts, size=EMBEDDING_DIM, window=5, min_count=5, workers=16, sg=0, negative=5)\n",
        "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NUM_WORDS))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd5kr29HfpCD"
      },
      "source": [
        "word_vectors = model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f-sC-6VgBPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62063c14-2ade-4a73-a51b-71d414f48ffd"
      },
      "source": [
        "# Masked padded sequences for training\n",
        "masked_sequences = np.array([[word_index.get(t, 0) for t in text]\n",
        "             for text in texts])\n",
        "masked_data = pad_sequences(masked_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXFf8-7xk_nD",
        "outputId": "fb5c450c-6f93-4593-aa0a-26649d441495"
      },
      "source": [
        "masked_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([135, 2379, 5, 203, 4, 294, 1, 32, 19, 114, 39, 295, 307, 14, 3908, 5, 82, 275, 1, 118, 47, 80, 8, 26, 4388, 928, 7, 31, 2, 164, 240, 5843, 2, 263, 5, 695, 10, 1, 37, 961, 6, 747, 3, 1787, 2980, 2, 9481, 74, 2, 84, 11, 6, 691, 3, 47, 80, 25, 1345, 4, 18, 378, 28, 1, 31, 2, 164, 356, 9, 1906, 31, 526, 546, 27, 8, 26, 2123, 1526, 7, 5843, 4, 346, 95, 99, 6, 158, 3, 197, 2, 369, 5, 203, 4, 1346, 1, 121, 3, 183, 2, 33, 216, 1036, 4, 335, 263, 2, 37, 77, 5, 203, 4, 1108, 389, 307, 241, 1, 39, 5, 21, 1198, 201, 2, 565, 601, 16, 10326, 14784, 25, 662, 6, 17, 4, 208, 6, 197, 2, 369, 427, 2572, 5, 280, 9, 31, 240, 2, 161, 263, 12, 14, 449, 798, 4, 1, 39, 5, 687, 5, 0, 695, 366, 2, 73, 1, 17, 327, 28, 258, 1, 2572, 3, 197, 2, 369, 314, 12, 4511, 4, 1, 957, 901, 5, 102, 106, 2, 273, 5, 695, 324, 234, 93, 2326, 4286, 88, 13, 2981, 1, 451, 2, 630, 2054, 13, 17, 101, 13, 2981, 1, 74, 2, 1692, 84, 4660, 2054, 13, 5843, 243, 9, 1, 84, 1660, 30, 669, 84, 533, 451, 1, 427, 2572, 330, 1312, 1, 623, 645, 3, 102, 0, 307, 2, 1, 229, 216, 1036, 5, 559, 1, 39, 2, 35, 137, 6, 597, 3679, 7, 1787, 84, 2, 74, 382, 103, 305, 2, 382, 103, 1, 121, 3, 197, 2, 369, 15, 78, 261, 18, 5094, 5, 6, 323, 7, 3545, 10, 1, 507, 691, 175, 35, 18, 11376, 350, 7, 455, 5, 6, 580, 3, 1625, 12, 45, 18, 6086, 2, 3909, 13, 295, 77, 142, 1, 161, 144, 1079, 2, 335, 35, 11, 17, 305, 552, 670, 268, 67, 314, 15, 18, 634, 5, 236, 608, 485, 3, 17698, 2, 12800, 201, 4, 1650, 1, 7295, 3, 1, 427, 2572, 2, 433, 27, 330, 35, 18, 3034, 4, 46, 37, 5843, 5, 695, 2, 37, 82, 5, 21, 307, 4, 294, 1, 39, 1, 184, 2572, 330, 686, 5843, 4, 1312, 2, 6341, 1, 305, 2, 603, 3, 21, 73, 2, 92, 241, 1, 39, 2, 137, 1086, 7, 574, 7, 231, 174, 27, 8, 382, 12, 55, 2, 55, 5843, 38, 14, 996, 5, 197, 2, 369, 3, 31, 47, 174, 2, 56, 1026, 31, 678, 3462, 15, 112, 1, 314, 4, 1028, 194, 21, 365, 276, 2, 92, 324, 160, 1099, 4, 1099, 216, 12, 21, 174, 241, 39, 330, 18, 389, 3463, 331, 122, 109, 14784, 56, 1959, 736, 17699, 1109, 11377, 1376, 8191]),\n",
              "       list([135, 132, 3, 42, 6087, 0, 17700, 10327, 25, 48, 365, 0, 0, 0, 0, 0, 38, 45, 18, 10328, 11, 6, 2419, 4, 1, 12801, 12, 15, 202, 26, 8753, 10, 1, 2205, 26, 3464, 3, 419, 11, 186, 11, 1, 12801, 38, 714, 18, 5271, 9, 34, 934, 2205, 33, 146, 79, 10, 1, 164, 2247, 139, 7, 10327, 661, 210, 4, 1, 19, 3, 6, 3993, 661, 2934, 1, 2506, 3, 3993, 2698, 2934, 1, 14785, 1804, 17701, 1804, 14786, 1804, 2, 17702, 1804, 102, 1804, 25, 48, 365, 3465, 2, 12802, 1, 14785, 1804, 646, 12, 1, 14785, 1804, 8, 6, 1804, 12, 8, 10328, 4, 6088, 1, 8192, 3, 2935, 1947, 26, 289, 3, 377, 1, 640, 1804, 8, 1, 17701, 1804, 11, 26, 289, 7, 10329, 775, 80, 217, 3615, 2, 598, 719, 9, 3274, 2, 10330, 688, 7, 2935, 1, 183, 1100, 4, 16, 1804, 8, 5, 1, 580, 3, 6, 3274, 2142, 9, 6, 52, 3, 3818, 2, 17703, 6, 5614, 6, 5844, 6, 3993, 80, 508, 2, 6, 5272, 1427, 1, 1062, 1804, 8, 1, 14786, 1804, 38, 8, 10328, 11, 6, 509, 289, 7, 1, 696, 3, 26, 880, 248, 898, 522, 27, 8, 1101, 6, 14786, 1594, 1370, 1960, 8, 248, 27, 8, 1087, 11, 26, 289, 4, 325, 2096, 4089, 2, 213, 80, 79, 10, 1, 3615, 925, 1, 289, 8, 2532, 120, 236, 1443, 2934, 1, 1394, 1466, 11, 6, 4389, 80, 289, 2, 1, 494, 1466, 11, 26, 880, 2124, 16, 1804, 8, 1, 2097, 908, 3, 6, 32, 775, 661, 522, 1, 126, 3, 258, 16, 289, 8, 1961, 453, 131, 13, 1, 10327, 661, 51, 88, 3, 38, 8, 30, 1, 0, 60, 2, 41, 627, 43, 269, 243, 9, 17704, 671, 7296, 7297, 1, 866, 1804, 8, 1, 17702, 1804, 38, 8, 10328, 11, 6, 17705, 1804, 38, 1847, 0, 73, 4, 1, 1626, 3, 12803, 0, 0, 4794, 16, 381, 5, 1, 10327, 661, 51, 8, 151, 9, 1, 671, 7296, 7297, 30, 1, 0, 0, 0, 6087, 469, 171, 4, 953, 131, 6, 60, 4, 301, 6, 213, 748, 1049, 12, 8, 253, 11, 6, 367, 7, 4090, 213, 80, 38, 45, 809, 1805, 3, 4389, 486, 943, 38, 8, 881, 13, 0, 17700, 1140, 213, 1313, 169, 16, 19, 220, 8, 724, 5, 1, 3275, 8754, 2815, 269, 991, 453, 131, 13, 17706, 24, 671, 7296, 7297, 5, 203, 4, 1820, 0, 8754, 2815, 49, 5450, 2, 4, 1136, 1, 1526, 3, 1, 17706, 7733, 164, 51, 5, 3400, 9, 43, 487, 30, 1043, 2, 351, 1043, 19, 4, 177, 1108, 19, 5, 1, 289, 13, 5615, 41, 1253, 162, 7, 52, 53, 2, 61, 119, 105, 129, 110, 136, 1, 105, 747, 329, 13, 1, 10327, 661, 51, 8, 1962, 95, 105, 86, 24, 10327, 661, 51, 2, 17704, 671, 7296, 7297]),\n",
              "       list([135, 1, 447, 168, 3, 16, 17, 8, 4, 194, 26, 856, 1007, 2507, 1, 418, 3, 123, 116, 120, 26, 430, 7, 1438, 2, 5616, 3, 307, 7, 32, 150, 2, 43, 19, 28, 1, 269, 3, 679, 335, 165, 65, 165, 65, 165, 0, 1063, 1, 17, 1325, 3, 53, 9, 679, 335, 685, 120, 1067, 1, 8193, 3, 152, 82, 581, 2936, 2, 909, 7, 108, 10, 1, 315, 1636, 165, 65, 303, 123, 554, 165, 198, 7, 361, 6342, 6342, 1, 2671, 3, 1, 118, 600, 1, 0, 0, 143, 771, 2736, 727, 2, 8755, 1833, 165, 198, 159, 361, 9, 1991, 4, 224, 3, 43, 2, 1386, 1963, 11, 90, 11, 319, 1963, 165, 65, 352, 339, 399, 165, 198, 1, 1007, 4, 6, 32, 150, 2, 43, 238, 9, 2, 7, 1, 765, 3, 1, 679, 867, 685, 120, 1067, 5845, 769, 2, 1206, 75, 246, 165, 198, 867, 123, 108, 7, 1, 765, 3, 1, 6343, 3, 6, 581, 679, 5095, 165, 65, 26, 679, 339, 944, 165, 198, 1, 1438, 3, 679, 434, 335, 293, 153, 240, 2, 4390, 679, 195, 4, 202, 1, 123, 1007, 6, 419, 7, 1, 765, 3, 679, 335, 165, 198, 1, 386, 3, 6, 411, 7, 1070, 2, 461, 570, 954, 165, 65, 165, 2573, 929, 151, 1194, 8, 767, 11, 2018, 165, 65, 1042, 4287, 2, 1452, 165, 2573, 825, 3, 16, 1042, 8, 4, 709, 6, 439, 4287, 3, 1, 229, 271, 5, 1, 33, 3, 1, 8756, 2, 1, 304, 5, 737, 3, 183, 2, 667, 165, 65, 1042, 523, 3, 1, 2937, 249, 165, 2573, 825, 3, 16, 1042, 8, 4, 463, 249, 4, 1, 487, 1872, 13, 123, 116, 165, 12804, 15, 1347, 1206, 1, 725, 7, 108, 13, 474, 293, 153, 2, 4390, 679, 181, 4, 527, 581, 679, 123, 108, 725, 45, 18, 600, 4, 1, 110, 126, 11, 6, 96, 1834, 5, 1, 33, 3, 26, 352, 867, 3335, 291, 2, 1, 560, 3, 1, 538, 283, 747, 16, 15, 976, 5, 687, 4, 1453, 120, 1, 12805, 2055, 16, 108, 15, 18, 453, 131, 5, 687, 9, 1, 46, 3, 1, 8757, 1, 6089, 2, 1, 4091, 165, 2573, 825, 3, 16, 58, 15, 18, 4, 208, 26, 535, 1167, 12, 425, 311, 1, 580, 3, 6, 867, 12806, 2816, 6, 581, 679, 475, 7, 352, 123, 108, 16, 6344, 15, 18, 79, 10, 1, 725, 2, 249, 615, 2, 3751, 9, 1, 225, 679, 276, 165, 65, 165, 65, 1042, 1558, 2, 1008, 165, 2573, 1062, 1042, 1325, 3, 3035, 1, 2327, 909, 3, 1, 839, 6344, 4, 1, 0, 454, 165, 2573, 151, 169, 15, 202, 6, 1558, 4, 1, 2143, 5, 203, 4, 1102, 1, 4795, 2, 48, 2982, 3546, 1, 17, 15, 18, 831, 4, 1, 439, 1873, 7, 1008, 165, 2573, 2608, 6344, 15, 18, 1, 96, 535, 2098, 3, 12805, 2, 30, 27, 679, 767, 293, 153, 7, 1, 33, 30, 569, 283, 3, 658, 2508, 4, 116, 1, 3401, 3, 123, 108, 13, 5273, 1, 867, 1246, 28, 1467, 367, 165, 65, 165, 65, 1042, 535, 2, 2983, 165, 12804, 866, 469, 330, 202, 27, 442, 4, 654, 1, 108, 4, 248, 760, 1, 10331, 4661, 3, 1, 0, 679, 293, 153, 175, 78, 261, 425, 112, 16, 12806, 4, 4391, 1, 295, 491, 10, 123, 108, 760, 1, 286, 3, 1, 12805, 382, 103, 303, 123, 554, 165, 65, 165, 65, 352, 339, 399, 165, 65, 165, 65, 26, 679, 339, 944, 331, 109, 12805, 361]),\n",
              "       ...,\n",
              "       list([135, 132, 3, 42, 1, 3268, 468, 4, 6091, 6612, 8, 1971, 131, 408, 4640, 108, 73, 12, 15, 137, 650, 2, 488, 4, 270, 104, 5, 1622, 1, 168, 8, 4, 2592, 1, 3630, 3, 6612, 2, 7994, 8629, 3, 4964, 216, 104, 45, 2316, 4, 21, 388, 3564, 251, 543, 1608, 2, 1563, 55, 2346, 3, 6612, 33, 146, 41, 4741, 351, 182, 670, 388, 5247, 2346, 562, 319, 766, 795, 338, 4640, 499, 50, 162, 7, 52, 53, 2, 61, 119, 949, 14, 665, 5, 2298, 1449, 7, 34, 155, 3, 1, 254, 75, 141, 14, 188, 662, 9, 2273, 263, 351, 3629, 2, 98, 391, 4, 1517, 358, 196, 84, 448, 2, 61, 119, 105, 129, 110, 136, 16, 42, 8, 4742, 13, 1, 3268, 51, 2, 1, 254, 3, 1620, 474, 1, 73, 14, 453, 131, 13, 1, 750, 3268, 0, 4, 6091, 6612, 11, 90, 11, 37, 1463, 9305, 181, 23, 14, 2274, 13, 1, 1, 384, 567, 3, 193, 1806, 3511, 0, 2, 1, 98, 86, 24, 3268, 468, 4, 6091, 6612]),\n",
              "       list([135, 1, 32, 19, 114, 7, 3714, 47, 7, 3714, 8, 6, 642, 379, 17, 42, 12, 327, 4, 325, 1, 341, 395, 39, 5, 40, 41, 13, 287, 1283, 249, 4, 31, 487, 239, 47, 40, 957, 1605, 11, 91, 5, 6, 106, 189, 3, 1, 337, 25, 198, 97, 4, 66, 8468, 384, 4, 865, 40, 366, 13, 8, 4, 443, 1, 260, 3, 49, 89, 178, 78, 29, 97, 4, 368, 30, 6, 60, 4, 527, 1, 170, 3, 368, 5, 3714, 30, 295, 92, 216, 23, 1641, 4, 1757, 16, 704, 13, 183, 2, 3201, 152, 92, 99, 11, 1, 1216, 3, 41, 5321, 5, 4315, 5417, 2, 336, 1216, 3342, 476, 9, 1979, 1512, 298, 2, 157, 1025, 2886, 69, 963, 10, 516, 5312, 38, 8, 6, 54, 66, 704, 5, 3714, 185, 468, 10, 1, 502, 3, 968, 1482, 2, 35, 10, 6492, 26, 535, 17, 9, 1, 51, 10, 555, 1, 44, 66, 910, 1, 254, 2, 183, 3, 67, 92, 36, 4200, 881, 2, 2912, 13, 91, 89, 546, 1922, 21, 1095, 4, 294, 2, 1395, 1028, 9, 181, 4, 294, 1, 366, 23, 15, 112, 16, 303, 1402, 2, 280, 52, 4, 1121, 40, 114, 11, 6, 642, 169, 23, 14, 90, 1444, 12, 40, 365, 130, 14, 78, 2040, 4, 1314, 1, 487, 4864, 38, 25, 1380, 384, 4, 301, 2932, 9, 351, 959, 181, 2, 460, 398, 5, 40, 41, 78, 261, 13, 560, 40, 581, 156, 175, 35, 40, 422, 2, 1764, 907, 5, 318, 4, 1730, 9, 31, 78, 7, 959, 181, 23, 35, 139, 4, 58, 5, 31, 51, 13, 1402, 9, 40, 6596, 193, 6596, 2, 94, 515, 672, 4, 389, 464, 40, 106, 5, 1, 98, 39, 2235, 23, 1415, 4, 18, 218, 3, 1, 3037, 3, 116, 2, 58, 2022, 37, 82, 4, 294, 67, 114, 23, 532, 4, 1925, 6, 2393, 103, 10, 40, 41, 2, 7, 1, 581, 90, 188, 3, 1, 231, 3, 1, 12162, 49, 2, 40, 62, 41, 11, 6, 851, 382, 103, 382, 103, 1216, 3, 41, 5321, 5, 1, 106, 3, 3660, 1442, 9723, 28, 1, 439, 3049, 1465, 5417, 3, 0, 0, 367, 2, 1, 0, 76, 20, 8867, 0, 350, 2, 722, 4, 722, 69, 7, 55, 200, 7430, 1211, 1153, 10, 1, 1242, 143, 0, 2, 0, 4052, 2, 35, 10, 516, 5312, 4, 1842, 601, 1, 2614, 3, 0, 5, 1, 2492, 2, 601, 516, 5312, 185, 3, 3, 1, 12162, 337, 10, 1, 502, 3, 797, 26, 1482, 3224, 2, 35, 6, 1302, 6492, 3224, 657, 4, 6, 75, 323, 3257, 13, 1, 118, 666, 1135, 1, 66, 144, 247, 55, 54, 417, 2, 300, 4316, 2710, 4, 245, 97, 4, 368, 7, 34, 16042, 251, 54, 130, 7, 66, 2989, 637, 2, 4266, 66, 368, 251, 1970, 97, 4, 170, 368, 331]),\n",
              "       list([135, 3964, 4, 527, 1, 1980, 3, 614, 2, 187, 1237, 97, 4, 554, 7, 34, 1, 250, 3, 554, 3, 1, 6558, 1196, 3, 2304, 7294, 15, 1395, 9, 1, 70, 505, 290, 10325, 4, 208, 6, 818, 505, 1025, 158, 12, 8, 874, 303, 32, 2, 4420, 256, 2304, 16, 1054, 15, 1108, 229, 241, 4057, 32, 19, 222, 27, 15, 1121, 33, 3, 2304, 24, 505, 1025, 1710, 13, 1074, 6, 505, 1025, 186, 500, 2, 708, 4, 187, 1105, 3, 303, 170, 3873, 4676, 134, 7, 706, 2, 541, 49, 2, 13, 53, 1, 52, 3, 1, 250, 3, 554, 505, 1025, 567, 1, 2108, 108, 15, 311, 364, 176, 6, 204, 824, 3, 262, 215, 22, 4, 9, 428, 256, 2304, 366, 46, 1, 303, 1216, 3, 2304, 24, 75, 505, 1025, 158, 301, 1, 52, 3, 1, 7294, 505, 1025, 567, 4, 137, 170, 505, 1025, 134, 951, 1, 1105, 3, 505, 1025, 5, 2223, 616, 2304, 86, 1116, 1, 7294, 8, 1, 51, 638, 376, 7, 1, 758, 3, 554, 5, 2304, 1, 7294, 24, 475, 8, 33, 3, 1, 1980, 3, 614, 2, 1237, 97, 4, 554, 7, 34, 1, 250, 3265, 6, 2648, 362, 5, 1105, 3, 505, 1025, 134, 4, 706, 2, 541, 726, 5, 2304, 2, 27, 15, 2100, 415, 233, 130, 688, 2, 1338, 46, 4, 611, 1, 19, 3, 6, 818, 505, 1025, 158, 12, 8, 874, 303, 32, 2, 4420, 7, 55, 74, 1362, 122, 7294, 1098, 5777, 128, 1, 10325, 8, 6, 3404, 254, 12, 401, 170, 3873, 4676, 134, 2, 1876, 32, 303, 505, 1025, 195, 275, 1, 118, 6, 668, 62, 1508, 7, 1, 822, 4, 9175, 1, 10325, 25, 176, 215, 3, 333, 1935, 505, 1025, 5, 2304, 2, 401, 182, 562, 1201, 27, 15, 137, 182, 562, 2, 1515, 446, 4, 1121, 16, 17, 7, 55, 74, 1362, 109, 0, 223, 33, 146, 33, 15, 284, 978, 467, 19, 87, 1412, 2, 19, 247, 1853, 69, 338, 1086, 2, 369, 2770, 2, 5386, 308, 73, 46, 1, 303, 1216, 3, 2304, 24, 75, 505, 1025, 158, 6, 9700, 1, 505, 1025, 186, 500, 2, 708, 540, 1736, 6, 44, 954, 10, 1942, 4, 2304, 24, 505, 1025, 158, 385, 2327, 6, 75, 44, 505, 1025, 291, 301, 1, 52, 3, 1, 7294, 505, 1025, 567, 4, 137, 170, 505, 1025, 6, 707, 6, 69, 807, 540, 707, 6, 197, 2, 369, 807, 385, 301, 1, 52, 3, 505, 1025, 567, 3700, 951, 1, 1105, 3, 505, 1025, 5, 2223, 616, 2304, 6, 707, 6, 984, 552, 6511, 747, 143, 1, 7294, 2, 10325, 540, 5621, 10325, 2304, 3700, 5, 7294, 505, 1025, 1282, 275, 1, 163, 595, 95, 129, 2, 110, 882, 583, 1340, 2, 26, 507, 678, 3, 7294, 505, 1025, 567, 390, 15, 18, 253, 4, 989, 1, 69, 807, 24, 103, 2, 229, 1, 505, 1025, 567, 197, 2, 369, 807, 15, 741, 1, 170, 3, 505, 1025, 343, 382, 103, 16, 1054, 15, 2590, 6, 818, 505, 1025, 158, 12, 8, 874, 303, 32, 2, 4420, 560, 195, 2, 4471, 1, 554, 1160, 7, 706, 104, 256, 2304, 280, 143, 1, 250, 3, 554, 3, 1, 6558, 1196, 3, 2304, 7294, 2, 1, 70, 505, 290, 10325, 15, 1108, 229, 241, 4057, 32, 19, 222, 16, 280, 15, 1121, 33, 3, 2304, 24, 505, 1025, 1710, 30, 182, 2, 219, 46, 7, 1, 505, 1025, 186, 500, 2, 708, 11, 90, 11, 53, 1, 52, 3, 1, 250, 3, 554, 505, 1025, 567, 5, 203, 4, 187, 1105, 3, 303, 170, 3873, 4676, 134, 7, 706, 2, 541, 49, 243, 1, 7294, 2, 10325, 15, 208, 6, 505, 1025, 158, 12, 8, 2206, 4, 1, 247, 3, 102, 6710, 2, 187, 1, 1105, 3, 170, 505, 1025, 134, 3220, 16, 1054, 15, 951, 97, 4, 505, 1025, 7, 8009, 56, 541, 49, 89, 14, 0, 16545, 5248, 3, 56, 6545, 9, 6, 3873, 0, 256, 2304, 331, 122, 7294, 1098, 5777, 128, 109, 0, 223])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy68j99gk4CZ",
        "outputId": "9f711371-62cd-4da3-b3ad-114efdf98a91"
      },
      "source": [
        "masked_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ..., 11377,  1376,  8191],\n",
              "       [    0,     0,     0, ...,   671,  7296,  7297],\n",
              "       [    2,  8755,  1833, ...,   109, 12805,   361],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,     4,  6091,  6612],\n",
              "       [    0,     0,     0, ...,   170,   368,   331],\n",
              "       [ 2304,     1,  7294, ...,   109,     0,   223]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj9niX53gNuh"
      },
      "source": [
        "mlb = MultiLabelBinarizer(classes = (\"goal_1\", \"goal_2\", \"goal_3\", \"goal_4\", \"goal_5\", \"goal_6\", \"goal_7\", \"goal_8\", \"goal_9\", \"goal_10\", \"goal_11\", \"goal_12\", \"goal_13\", \"goal_14\", \"goal_15\", \"goal_16\", \"goal_17\"))\n",
        "#create boolean mask matched non NaNs values\n",
        "mask = labelled['Tag'].notnull()\n",
        "\n",
        "labels = np.array(mlb.fit_transform(labelled.loc[mask, 'Tag'].dropna().str.strip('[]').str.split(',')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM9uelsyha08",
        "outputId": "5b9605b3-6b34-4b99-93f2-c79d652f4765"
      },
      "source": [
        "models = []\n",
        "arch = 'Conv1D_glorot_uniform'\n",
        "is_mask = \"masked\"\n",
        "\n",
        "for fold in os.listdir(CROSS_FOLDS):\n",
        "    train_index = np.load(f\"{CROSS_FOLDS}{fold}/train.npy\")\n",
        "    val_index = np.load(f\"{CROSS_FOLDS}{fold}/val.npy\")\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_train, x_val, x_test = masked_data[train_index], masked_data[val_index], masked_data[test_index]\n",
        "    y_train, y_val, y_test = labels[train_index], labels[val_index], labels[test_index]\n",
        "    \n",
        "        \n",
        "    print(F\"Training {fold}\")\n",
        "\n",
        "    print('Preparing embedding matrix.')\n",
        "    # prepare embedding matrix\n",
        "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    \n",
        "    for word, i in word_index.items():\n",
        "        if i > MAX_NUM_WORDS:\n",
        "            continue\n",
        "        try:\n",
        "            embedding_vector = word_vectors[word]\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except:\n",
        "            pass   \n",
        "    \n",
        "    # load pre-trained word embeddings into an Embedding layer\n",
        "    # note that we set trainable = False so as to keep the embeddings fixed\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                                EMBEDDING_DIM,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "    \n",
        "    print('Training model.')\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    # 0.22\n",
        "    if arch == 'conv': \n",
        "        # 1D convnet with global maxpooling\n",
        "        x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = MaxPooling1D(5)(x)\n",
        "        x = Conv1D(128, 5, activation='relu')(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                    optimizer=Adam(lr=0.01), \n",
        "                    metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "    # 0.16, 8 epochs without Bidirectional\n",
        "    # 0.15, 8 epochs with Bidirectional\n",
        "    # 0.13, 10 epochs with Bidirectional\n",
        "    if arch == \"bidirectionalGRU\":\n",
        "        x = Bidirectional(GRU(128, return_sequences=True, dropout=0.1,recurrent_dropout=0.1))(embedded_sequences)\n",
        "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = concatenate([avg_pool, max_pool])\n",
        "        preds = Dense(17, activation=\"sigmoid\")(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.summary() \n",
        "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "     # around .21, 10 epochs with Bidirectional\n",
        "    if arch == \"Bidirectional_LSTM\":\n",
        "        x = Bidirectional(LSTM(25, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedded_sequences)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(50, activation=\"relu\")(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(17, activation=\"sigmoid\")(x)\n",
        "        model = Model(inputs=sequence_input, outputs=x)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "        \n",
        "        \n",
        "    if arch == \"Conv1D_glorot_uniform\":\n",
        "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(embedded_sequences)\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        x = concatenate([avg_pool, max_pool])\n",
        "        preds = Dense(len(labels_index), activation='sigmoid')(x)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                #optimizer=Adam(lr=0.001),\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(x_train, y_train,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=NUM_EPOCHS,\n",
        "            validation_data=(x_val, y_val))\n",
        "\n",
        "    models.append([model, x_test, y_test])\n",
        "    model.save(EMBEDDINGS_DIR + f\"{is_mask}{arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\")\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training fold_1\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Epoch 1/20\n",
            "11/11 [==============================] - 6s 493ms/step - loss: 0.6979 - accuracy: 0.0692 - val_loss: 0.5458 - val_accuracy: 0.1272\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 5s 450ms/step - loss: 0.5545 - accuracy: 0.1315 - val_loss: 0.5375 - val_accuracy: 0.1272\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 5s 450ms/step - loss: 0.5404 - accuracy: 0.1189 - val_loss: 0.5358 - val_accuracy: 0.0751\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 5s 447ms/step - loss: 0.5338 - accuracy: 0.1023 - val_loss: 0.5333 - val_accuracy: 0.1272\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 5s 447ms/step - loss: 0.5409 - accuracy: 0.1537 - val_loss: 0.5285 - val_accuracy: 0.1098\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 5s 450ms/step - loss: 0.5306 - accuracy: 0.1341 - val_loss: 0.5369 - val_accuracy: 0.1850\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 5s 451ms/step - loss: 0.5220 - accuracy: 0.1906 - val_loss: 0.5293 - val_accuracy: 0.0636\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 5s 451ms/step - loss: 0.5270 - accuracy: 0.1307 - val_loss: 0.5334 - val_accuracy: 0.0925\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 5s 448ms/step - loss: 0.5076 - accuracy: 0.1492 - val_loss: 0.5277 - val_accuracy: 0.0867\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 5s 446ms/step - loss: 0.5215 - accuracy: 0.1430 - val_loss: 0.5245 - val_accuracy: 0.1272\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 5s 449ms/step - loss: 0.5018 - accuracy: 0.1564 - val_loss: 0.5275 - val_accuracy: 0.0925\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 5s 450ms/step - loss: 0.5049 - accuracy: 0.1772 - val_loss: 0.5336 - val_accuracy: 0.0983\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 5s 449ms/step - loss: 0.4999 - accuracy: 0.1753 - val_loss: 0.5354 - val_accuracy: 0.1098\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 5s 451ms/step - loss: 0.4950 - accuracy: 0.1833 - val_loss: 0.5324 - val_accuracy: 0.1734\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 5s 450ms/step - loss: 0.4947 - accuracy: 0.1701 - val_loss: 0.5579 - val_accuracy: 0.1040\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 5s 467ms/step - loss: 0.4917 - accuracy: 0.1961 - val_loss: 0.5278 - val_accuracy: 0.0867\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 5s 446ms/step - loss: 0.4712 - accuracy: 0.1836 - val_loss: 0.5431 - val_accuracy: 0.0925\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 5s 450ms/step - loss: 0.4949 - accuracy: 0.2156 - val_loss: 0.5344 - val_accuracy: 0.0809\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 5s 453ms/step - loss: 0.4822 - accuracy: 0.1944 - val_loss: 0.5307 - val_accuracy: 0.1503\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 5s 454ms/step - loss: 0.4738 - accuracy: 0.2103 - val_loss: 0.5385 - val_accuracy: 0.1156\n",
            "Training fold_2\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Epoch 1/20\n",
            "11/11 [==============================] - 6s 488ms/step - loss: 0.6748 - accuracy: 0.0699 - val_loss: 0.5371 - val_accuracy: 0.0936\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 5s 442ms/step - loss: 0.5565 - accuracy: 0.1183 - val_loss: 0.5339 - val_accuracy: 0.1287\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 5s 441ms/step - loss: 0.5411 - accuracy: 0.1216 - val_loss: 0.5357 - val_accuracy: 0.0994\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 5s 438ms/step - loss: 0.5447 - accuracy: 0.1139 - val_loss: 0.5440 - val_accuracy: 0.0760\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 5s 441ms/step - loss: 0.5419 - accuracy: 0.1438 - val_loss: 0.5273 - val_accuracy: 0.0936\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5316 - accuracy: 0.1493 - val_loss: 0.5292 - val_accuracy: 0.0760\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5181 - accuracy: 0.1637 - val_loss: 0.5307 - val_accuracy: 0.1053\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 5s 439ms/step - loss: 0.5236 - accuracy: 0.1732 - val_loss: 0.5398 - val_accuracy: 0.0643\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 5s 442ms/step - loss: 0.5114 - accuracy: 0.1687 - val_loss: 0.5316 - val_accuracy: 0.0936\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 5s 442ms/step - loss: 0.5124 - accuracy: 0.1659 - val_loss: 0.5426 - val_accuracy: 0.0994\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 5s 446ms/step - loss: 0.5091 - accuracy: 0.1861 - val_loss: 0.5541 - val_accuracy: 0.0819\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 5s 437ms/step - loss: 0.5076 - accuracy: 0.1737 - val_loss: 0.5301 - val_accuracy: 0.1228\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5080 - accuracy: 0.1835 - val_loss: 0.5353 - val_accuracy: 0.0702\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5066 - accuracy: 0.1589 - val_loss: 0.5383 - val_accuracy: 0.0760\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 5s 446ms/step - loss: 0.4858 - accuracy: 0.1741 - val_loss: 0.5425 - val_accuracy: 0.1111\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 5s 456ms/step - loss: 0.4937 - accuracy: 0.1874 - val_loss: 0.5553 - val_accuracy: 0.0994\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 5s 443ms/step - loss: 0.4998 - accuracy: 0.1668 - val_loss: 0.5377 - val_accuracy: 0.1053\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 5s 437ms/step - loss: 0.4957 - accuracy: 0.2104 - val_loss: 0.5354 - val_accuracy: 0.0936\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 5s 441ms/step - loss: 0.4916 - accuracy: 0.1886 - val_loss: 0.5453 - val_accuracy: 0.1696\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 5s 441ms/step - loss: 0.4989 - accuracy: 0.1980 - val_loss: 0.5354 - val_accuracy: 0.1170\n",
            "Training fold_3\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Epoch 1/20\n",
            "11/11 [==============================] - 6s 487ms/step - loss: 0.6418 - accuracy: 0.0783 - val_loss: 0.5743 - val_accuracy: 0.0529\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 5s 445ms/step - loss: 0.5479 - accuracy: 0.0952 - val_loss: 0.5850 - val_accuracy: 0.0882\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 5s 441ms/step - loss: 0.5395 - accuracy: 0.0979 - val_loss: 0.5676 - val_accuracy: 0.1176\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 5s 446ms/step - loss: 0.5355 - accuracy: 0.1284 - val_loss: 0.5676 - val_accuracy: 0.0588\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 5s 439ms/step - loss: 0.5376 - accuracy: 0.1233 - val_loss: 0.5711 - val_accuracy: 0.0706\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 5s 439ms/step - loss: 0.5193 - accuracy: 0.1336 - val_loss: 0.5745 - val_accuracy: 0.1000\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 5s 435ms/step - loss: 0.5221 - accuracy: 0.1505 - val_loss: 0.5671 - val_accuracy: 0.0706\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 5s 436ms/step - loss: 0.5094 - accuracy: 0.1554 - val_loss: 0.5742 - val_accuracy: 0.0765\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 5s 436ms/step - loss: 0.5068 - accuracy: 0.1534 - val_loss: 0.5667 - val_accuracy: 0.1118\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5188 - accuracy: 0.1568 - val_loss: 0.5762 - val_accuracy: 0.1176\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 5s 436ms/step - loss: 0.4964 - accuracy: 0.1915 - val_loss: 0.5680 - val_accuracy: 0.1294\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 5s 436ms/step - loss: 0.4986 - accuracy: 0.1890 - val_loss: 0.5721 - val_accuracy: 0.0647\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 5s 437ms/step - loss: 0.4933 - accuracy: 0.1792 - val_loss: 0.5727 - val_accuracy: 0.1647\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 5s 438ms/step - loss: 0.4965 - accuracy: 0.2154 - val_loss: 0.5796 - val_accuracy: 0.0941\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 5s 434ms/step - loss: 0.4892 - accuracy: 0.1870 - val_loss: 0.5784 - val_accuracy: 0.1000\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 5s 454ms/step - loss: 0.4931 - accuracy: 0.1882 - val_loss: 0.5734 - val_accuracy: 0.0706\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 5s 434ms/step - loss: 0.4950 - accuracy: 0.2027 - val_loss: 0.5748 - val_accuracy: 0.1471\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 5s 437ms/step - loss: 0.4834 - accuracy: 0.1904 - val_loss: 0.5779 - val_accuracy: 0.0882\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 5s 434ms/step - loss: 0.4832 - accuracy: 0.1960 - val_loss: 0.5782 - val_accuracy: 0.1000\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 5s 436ms/step - loss: 0.4819 - accuracy: 0.2112 - val_loss: 0.5778 - val_accuracy: 0.0765\n",
            "Training fold_4\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Epoch 1/20\n",
            "11/11 [==============================] - 6s 486ms/step - loss: 0.7170 - accuracy: 0.0643 - val_loss: 0.5989 - val_accuracy: 0.1176\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 5s 443ms/step - loss: 0.5499 - accuracy: 0.0989 - val_loss: 0.5949 - val_accuracy: 0.1353\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 5s 444ms/step - loss: 0.5370 - accuracy: 0.1228 - val_loss: 0.5884 - val_accuracy: 0.0882\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 5s 441ms/step - loss: 0.5260 - accuracy: 0.1379 - val_loss: 0.5877 - val_accuracy: 0.0824\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 5s 447ms/step - loss: 0.5331 - accuracy: 0.1257 - val_loss: 0.5948 - val_accuracy: 0.1059\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 7s 687ms/step - loss: 0.5286 - accuracy: 0.1459 - val_loss: 0.5901 - val_accuracy: 0.1235\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 7s 647ms/step - loss: 0.5234 - accuracy: 0.1600 - val_loss: 0.5908 - val_accuracy: 0.0824\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 5s 445ms/step - loss: 0.5137 - accuracy: 0.1711 - val_loss: 0.5899 - val_accuracy: 0.1294\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 5s 443ms/step - loss: 0.5097 - accuracy: 0.1937 - val_loss: 0.5910 - val_accuracy: 0.1235\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 5s 448ms/step - loss: 0.5054 - accuracy: 0.1928 - val_loss: 0.5886 - val_accuracy: 0.1000\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 5s 442ms/step - loss: 0.5001 - accuracy: 0.1852 - val_loss: 0.6065 - val_accuracy: 0.1588\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 5s 445ms/step - loss: 0.4935 - accuracy: 0.2180 - val_loss: 0.5996 - val_accuracy: 0.1353\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 5s 445ms/step - loss: 0.4975 - accuracy: 0.1979 - val_loss: 0.5929 - val_accuracy: 0.1235\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 5s 446ms/step - loss: 0.4836 - accuracy: 0.1969 - val_loss: 0.5992 - val_accuracy: 0.1235\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 5s 448ms/step - loss: 0.4906 - accuracy: 0.1932 - val_loss: 0.6067 - val_accuracy: 0.1706\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 5s 446ms/step - loss: 0.4822 - accuracy: 0.2146 - val_loss: 0.5997 - val_accuracy: 0.1118\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 5s 460ms/step - loss: 0.4815 - accuracy: 0.1981 - val_loss: 0.5993 - val_accuracy: 0.1000\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 5s 446ms/step - loss: 0.4806 - accuracy: 0.2002 - val_loss: 0.5970 - val_accuracy: 0.1118\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 5s 445ms/step - loss: 0.4683 - accuracy: 0.1853 - val_loss: 0.6097 - val_accuracy: 0.1412\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 5s 449ms/step - loss: 0.4795 - accuracy: 0.2264 - val_loss: 0.6160 - val_accuracy: 0.1235\n",
            "Training fold_5\n",
            "Preparing embedding matrix.\n",
            "Training model.\n",
            "Epoch 1/20\n",
            "11/11 [==============================] - 6s 487ms/step - loss: 0.7228 - accuracy: 0.0690 - val_loss: 0.5744 - val_accuracy: 0.0655\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 5s 438ms/step - loss: 0.5620 - accuracy: 0.0927 - val_loss: 0.5774 - val_accuracy: 0.0714\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 5s 441ms/step - loss: 0.5513 - accuracy: 0.1103 - val_loss: 0.5684 - val_accuracy: 0.1071\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5395 - accuracy: 0.1175 - val_loss: 0.5683 - val_accuracy: 0.1131\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 5s 441ms/step - loss: 0.5381 - accuracy: 0.1456 - val_loss: 0.5798 - val_accuracy: 0.1667\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 5s 439ms/step - loss: 0.5353 - accuracy: 0.1486 - val_loss: 0.5660 - val_accuracy: 0.1369\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 5s 442ms/step - loss: 0.5396 - accuracy: 0.1499 - val_loss: 0.5711 - val_accuracy: 0.1548\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 5s 436ms/step - loss: 0.5203 - accuracy: 0.1764 - val_loss: 0.5688 - val_accuracy: 0.0952\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5320 - accuracy: 0.1638 - val_loss: 0.5702 - val_accuracy: 0.0893\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5170 - accuracy: 0.1678 - val_loss: 0.5672 - val_accuracy: 0.1429\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5008 - accuracy: 0.1757 - val_loss: 0.5753 - val_accuracy: 0.1369\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 5s 439ms/step - loss: 0.5047 - accuracy: 0.1814 - val_loss: 0.5682 - val_accuracy: 0.0952\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5180 - accuracy: 0.1634 - val_loss: 0.5767 - val_accuracy: 0.1369\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 5s 437ms/step - loss: 0.5018 - accuracy: 0.1882 - val_loss: 0.5816 - val_accuracy: 0.1369\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.5011 - accuracy: 0.1826 - val_loss: 0.5726 - val_accuracy: 0.1131\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 5s 441ms/step - loss: 0.4935 - accuracy: 0.1787 - val_loss: 0.5711 - val_accuracy: 0.1786\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 5s 442ms/step - loss: 0.4860 - accuracy: 0.2195 - val_loss: 0.5712 - val_accuracy: 0.1548\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 5s 440ms/step - loss: 0.4883 - accuracy: 0.2024 - val_loss: 0.5876 - val_accuracy: 0.1310\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 5s 461ms/step - loss: 0.4872 - accuracy: 0.2222 - val_loss: 0.5937 - val_accuracy: 0.1071\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 5s 439ms/step - loss: 0.4913 - accuracy: 0.2248 - val_loss: 0.5790 - val_accuracy: 0.1548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spCHYQLThbPu"
      },
      "source": [
        "\n",
        "def metrics_avg(models_testx_testy, labels_, thres=0.3):\n",
        "    def calc(model, test_x, test_y):\n",
        "        predictions = model.predict(test_x)>thres\n",
        "        metrics = classification_report(test_y, predictions, target_names=labels_, output_dict=True)\n",
        "        metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "        h = hamming_loss(test_y, predictions)\n",
        "        roc = roc_auc_score(test_y, predictions, average='micro')\n",
        "        return metrics_df, h, roc\n",
        "\n",
        "    model_1, test_x_first, test_y_first = models_testx_testy[0]\n",
        "    metrics_agg, ham, roc = calc(model_1, test_x_first, test_y_first)\n",
        "    n = len(models_testx_testy)\n",
        "\n",
        "    for model, test_x, test_y in models_testx_testy[1:]:\n",
        "        metrics, h, r = calc(model, test_x, test_y)\n",
        "        metrics_agg += metrics\n",
        "        ham += h\n",
        "        roc += r\n",
        "\n",
        "    return metrics_agg/n, ham/n, roc/n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "EYp4sywnhbEk",
        "outputId": "4fce7832-cccc-4c49-c1d4-a59d37072b94"
      },
      "source": [
        "loaded_arch = 'maskedConv1D_glorot_uniform'\n",
        "loaded_models = []\n",
        "final_models = []\n",
        "for i, fold in enumerate(os.listdir(CROSS_FOLDS)):\n",
        "    print(f\"Loading {fold}...\")\n",
        "    test_index = np.load(f\"{CROSS_FOLDS}{fold}/test.npy\")\n",
        "\n",
        "    x_test = masked_data[test_index]\n",
        "    y_test = labels[test_index]\n",
        "    \n",
        "    load_dir = EMBEDDINGS_DIR + f\"{loaded_arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\"\n",
        "    \n",
        "    final_models.append((loaded_models[i], x_test, y_test))\n",
        "print(f\"Finished loading the {loaded_arch} models.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading fold_1...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a9991109ab48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mload_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMBEDDINGS_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"{loaded_arch}_{NUM_EPOCHS}epochs_{EMBEDDING_DIM}D_batchsize{BATCH_SIZE}_5fold-cross-val_{fold}.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfinal_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Finished loading the {loaded_arch} models.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emoDrka1iWgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b530bba-ba8a-4a77-b2a0-e3a9a5a535bc"
      },
      "source": [
        "avg_results = metrics_avg(models, labels_index, thres=0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxf-3SuziWcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "83d1e75a-e766-4c4e-8e1c-0bd1aab1c7fc"
      },
      "source": [
        "avg_results[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>micro avg</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>weighted avg</th>\n",
              "      <th>samples avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.313880</td>\n",
              "      <td>0.230229</td>\n",
              "      <td>0.291460</td>\n",
              "      <td>0.410082</td>\n",
              "      <td>0.316481</td>\n",
              "      <td>0.243094</td>\n",
              "      <td>0.276805</td>\n",
              "      <td>0.334438</td>\n",
              "      <td>0.213759</td>\n",
              "      <td>0.226546</td>\n",
              "      <td>0.316958</td>\n",
              "      <td>0.299390</td>\n",
              "      <td>0.297540</td>\n",
              "      <td>0.171770</td>\n",
              "      <td>0.250106</td>\n",
              "      <td>0.282028</td>\n",
              "      <td>0.428877</td>\n",
              "      <td>0.319589</td>\n",
              "      <td>0.288438</td>\n",
              "      <td>0.304945</td>\n",
              "      <td>0.324689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.617798</td>\n",
              "      <td>0.275868</td>\n",
              "      <td>0.441237</td>\n",
              "      <td>0.761963</td>\n",
              "      <td>0.509910</td>\n",
              "      <td>0.321677</td>\n",
              "      <td>0.383333</td>\n",
              "      <td>0.766928</td>\n",
              "      <td>0.160911</td>\n",
              "      <td>0.336842</td>\n",
              "      <td>0.401052</td>\n",
              "      <td>0.388439</td>\n",
              "      <td>0.502349</td>\n",
              "      <td>0.070588</td>\n",
              "      <td>0.308292</td>\n",
              "      <td>0.393789</td>\n",
              "      <td>0.945092</td>\n",
              "      <td>0.502759</td>\n",
              "      <td>0.446239</td>\n",
              "      <td>0.502759</td>\n",
              "      <td>0.540859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.412096</td>\n",
              "      <td>0.232940</td>\n",
              "      <td>0.313526</td>\n",
              "      <td>0.530061</td>\n",
              "      <td>0.376215</td>\n",
              "      <td>0.249995</td>\n",
              "      <td>0.316556</td>\n",
              "      <td>0.461090</td>\n",
              "      <td>0.181724</td>\n",
              "      <td>0.236786</td>\n",
              "      <td>0.338333</td>\n",
              "      <td>0.322345</td>\n",
              "      <td>0.354064</td>\n",
              "      <td>0.075101</td>\n",
              "      <td>0.262982</td>\n",
              "      <td>0.325708</td>\n",
              "      <td>0.589433</td>\n",
              "      <td>0.387543</td>\n",
              "      <td>0.328174</td>\n",
              "      <td>0.359908</td>\n",
              "      <td>0.336721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>109.800000</td>\n",
              "      <td>88.600000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>139.600000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>79.600000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>120.200000</td>\n",
              "      <td>69.600000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>96.200000</td>\n",
              "      <td>89.600000</td>\n",
              "      <td>114.400000</td>\n",
              "      <td>50.800000</td>\n",
              "      <td>77.200000</td>\n",
              "      <td>91.400000</td>\n",
              "      <td>163.800000</td>\n",
              "      <td>1658.800000</td>\n",
              "      <td>1658.800000</td>\n",
              "      <td>1658.800000</td>\n",
              "      <td>1658.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    1          2  ...  weighted avg  samples avg\n",
              "precision    0.313880   0.230229  ...      0.304945     0.324689\n",
              "recall       0.617798   0.275868  ...      0.502759     0.540859\n",
              "f1-score     0.412096   0.232940  ...      0.359908     0.336721\n",
              "support    109.800000  88.600000  ...   1658.800000  1658.800000\n",
              "\n",
              "[4 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJe0haG8ibGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c90e5142-4f42-4ca7-d53f-a6c07e4a6fa8"
      },
      "source": [
        "hl = round(avg_results[1],4)\n",
        "roc_auc = round(avg_results[2],4)\n",
        "print(f\"hl;{hl}\")\n",
        "print(f\"roc-auc;{roc_auc}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hl;0.3984\n",
            "roc-auc;0.5692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itcp4QTAia_0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s6RaWZqCtCr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}